---
title: "TM02_news_typhoon"
author: "Jilung Hsieh"
date: "2018/7/3"
output: 
  html_document: 
    number_sections: true
    highlight: textmate
    theme: spacelab
    toc: yes
editor_options: 
  chunk_output_type: inline
---

# 載入套件

1. tidyverse內涵繪圖和操作資料所需要的ggplot2和dplyr
2. stringr雖然隨著tidyverse被安裝了，但不會隨著tidyverse被載入，要另外載入。
3. 在中文斷詞的時候要用到tidytext和jiebaR。
4. 處理時間時要用到lubridate。

```{r}
library(tidyverse)
library(stringr)
library(tidytext)
library(jiebaR)
library(lubridate)
```

# 為文件編id

* 為了便於後續建立Document-Term-Matrix，這時候若Document自身沒有編號的話，就得把整個Document內容當成該篇文章的id，但也有（極小）可能有兩篇Document內容相同，那就會被視為同一篇文章或發生錯誤。所以必須要編id。
* `row_number()`產生每列的編號，所以這邊就直接把每列的編號視為每篇文章的id，可以保持該id的唯一性。

```{r}
news.df <- readRDS("data/typhoon.rds") %>%
    mutate(doc_id = row_number())
```


# 斷詞

## 初始化斷詞器`cutter <- worker()`

1. 斷詞的時候不見能把我們要的字詞斷出來，比方說你可能希望台北市不會被斷開，偏偏被斷成台北+市。請參見謝舒凱老師的講義。https://legacy.gitbook.com/book/loperntu/ladsbook/details。最簡單的方法就是窮舉，例如下面的`segment_not`即是。
2. 初始化斷詞器後，


```{r}
segment_not <- c("第卅六條", "第卅八條", "蘇南成", "災前", "災後", "莫拉克", "颱風", "應變中心", "停班停課", "停課", "停班", "停駛", "路樹", "里長", "賀伯", "採收", "菜價", "蘇迪", "受災戶", "颱風警報", "韋恩", "台東縣", "馬總統", "豪大雨", "梅姬", "台東", "台北市政府", "工務段", "漂流木", "陳菊", "台南縣", "卡玫基", "魚塭", "救助金", "陳情", "全省", "強颱", "中颱", "輕颱", "小林村", "野溪", "蚵民", "農委會", "來襲", "中油公司", "蔣總統經國", "颱風天", "土石流", "蘇迪勒", "水利署", "陳說", "颱風假", "颱風地區", "台灣", "臺灣", "柯羅莎", "八八風災", "紓困","傅崑萁", "傅崐萁","台中", "文旦柚", "鄉鎮市公所", "鄉鎮市", "房屋稅", "高雄", "未達", "台灣省", "台北市")
cutter <- worker()
new_user_word(cutter, segment_not)
stopWords <- readRDS("data/stopWords.rds")
```

```{r}
news.df$time %>% summary

tokenized.df <- news.df %>%
    mutate(timestamp=ymd(time)) %>% 
    # filter(timestamp > as.Date("2009-01-01")) %>%
    select(-time) %>%
    select(title, text, cat, timestamp, everything()) %>%
    mutate(word = purrr::map(text, function(x)segment(x, cutter)))

unnested.df <- tokenized.df %>%
    select(doc_id, text, word) %>%
    unnest(word) %>%
    filter(!(word %in% stopWords$word)) %>%
    filter(!str_detect(word, "[a-zA-Z0-9]+"))
```


# bigrams 建立文字共現網絡

```{r}

bigrams <- unnested.df %>%
    select(doc_id, w1=word) %>%
    group_by(doc_id) %>%
    mutate(w2 = lead(w1, 1)) %>%
    ungroup() %>%
    filter(complete.cases(.))


bigrams.count <- bigrams %>%
    count(w1, w2, sort = T)

# Time-consumed
bigram.tf_idf <- bigrams %>%
    unite(bigram, w1, w2, sep = " ") %>%
    count(doc_id, bigram) %>%
    bind_tf_idf(bigram, doc_id, n) %>%
    arrange(desc(n))


bigram.tf_idf %>%
    ggplot(aes(tf_idf)) +
    geom_histogram(bins = 100)  + 
    scale_x_log10()
```



# 建立網絡bigram

## textmining with R version

```
library(igraph)
bigram_graph <- bigrams.count %>%
  filter(n > 20) %>%
  graph_from_data_frame()

library(ggraph)
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1, family="Heiti TC Light")

```


## Workable 
```{r}

library(igraph)

# bigrams.count %>%
#     count(n) %>%
#     ggplot(aes(n, nn)) +
#     geom_point() + 
#     scale_x_log10() + 
#     scale_y_log10()

toplot <- bigrams.count %>%
  slice(1:500)
g <- graph.data.frame(toplot,directed = FALSE) 
V(g)$size = log(centralization.degree(g)$res + 1)
E(g)$width = toplot$n
g <- simplify(g, remove.loops=T)
l <- layout_with_kk(g)
# l <- layout_with_mds(g)  
l <- layout.fruchterman.reingold(g)
E(g)$width

plot(g, vertex.label = V(g)$name,  
     edge.curved = 0.2,
     vertex.label.cex = 0.6,
     vertex.size  = sqrt(V(g)$size)/10,
     edge.arrow.size = 0, 
     layout = l,
     edge.width = log10(E(g)$width-min(E(g)$width) + 1),
     # edge.width=log10(E(g)$weight),
     vertex.label.family = 'Heiti TC Light',
     edge.color = rgb(0.5,0.5,0.5,0.5))



```

# 練習：根據前5000大tf-idf建立網絡會是什麼樣的情形？


# truncated unnest_token() only useful for EN

```
bigrams <- recombined.df %>%
  unnest_tokens(bigram, string, token = "ngrams", n = 2, drop = F)

bigrams %>% head(n=200) %>% View()

bigrams %>%
    count(bigram, sort = T) %>% 
    slice(1:500) %>%
    View()

bigrams.separated <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams.separated %>% head() %>% View()

bigrams.filtered <- bigrams.separated %>%
    filter(word1 != word2) %>%
    filter(!word1 %in% stopWords$word) %>%
    filter(!word2 %in% stopWords$word)

bigrams.filtered %>%
    count(word1, word2, sort = T) %>%
    View()

```


# unnested -> LDA
```
dtm <- unnested.df %>%
    count(doc_id, word) %>%
    tidytext::cast_dtm(doc_id, word, n)
lda16 <- LDA(dtm, k = 12, control = list(seed = 1234))
```

```
doc_topics <- tidy(lda16, matrix = "gamma") %>%
	spread(topic, gamma)

t <- tidy(lda16, matrix = "gamma") %>%
    mutate(doc_id = as.numeric(document)) %>%
    select(-document) %>%
    left_join(news.df %>% select(doc_id, cat), by = "doc_id") %>%
    filter(topic == 5) %>%
    t.test(gamma ~ cat, data = .)

tidy(lda16, matrix = "gamma") %>%
    mutate(doc_id = as.numeric(document)) %>%
    select(-document) %>%
    left_join(news.df %>% select(doc_id, cat), by = "doc_id") %>%
    group_by(topic) %>%
    summarize(p.value = t.test(gamma ~ cat) %>% .$p.value,
              t = t.test(gamma ~ cat) %>% .$statistic) %>%
    ungroup()
    # filter(topic == 4) %>%
    # t.test(gamma ~ cat, data = .) %>%
    # .$p.value
```

